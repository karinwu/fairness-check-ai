This package is a Python toolkit for testing bias and assessing fairness in AI and predictive models. It is designed to help data scientists, ML engineers, and risk/compliance teams systematically evaluate whether model outcomes treat different groups fairly.

The library focuses on models built from both traditional data sources and External Consumer Data and Information sources (ECDIS), including systems that use machine learning and broader artificial intelligence techniques. It provides an integrated workflow to detect potential unfair discrimination, compute group-level fairness metrics, and summarize results in a way that is suitable for model governance and documentation.

With fairness-check-ai, you can:

- Define protected and comparison groups (e.g., by demographic or behavioral attributes).
- Compute common fairness and bias indicators across classification and regression models.
- Compare model performance and outcomes across groups to highlight potential disparities.
- Integrate fairness checks into your model development, validation, and monitoring pipelines.

This repository aims to make fairness evaluation repeatable, transparent, and easy to adopt as part of responsible AI practices.
